{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "212edb18-fefb-4cd8-b16e-1977fd3eba9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LLM migration driven by data-aware prompt optimization - RAG-QA task\n",
    "\n",
    "Example notebook \n",
    "- model migration from Claude 3 Haiku to Nova-Lite for a question-answer task\n",
    "- optimization by Bedrock APO and data-aware optimization after migration\n",
    "- evaluation metric: semantic similarity  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562d489e-dd83-4ca5-9cc7-562b9c843192",
   "metadata": {},
   "source": [
    "### Install python packages requested by benchmarking\n",
    "\n",
    "If you have not install the requested python libraries, uncomment the following command to run the installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04a0083-fc55-4630-a1ca-20437b4d8057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r ../peccyben/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25536b8-4f4f-490f-9824-4b91bc0340bf",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5c911c-4fea-4779-8c70-c8e78308814e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os, time\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json, copy\n",
    "import warnings \n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from peccymig.migration import Prompt_Opt_Template_Gen, eval_metric_ss\n",
    "from peccymig.migration import model_initialize, model_inference, data_aware_optimization, update_prompt_catalog\n",
    "from peccyben.qaragtask import QA_Ben, QA_Create_VDB \n",
    "from peccyben.promptcatalog import Prompt_Template_Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abed4555-bda8-4ad3-9916-5d4166514d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "boto3_bedrock = boto3.client(service_name=\"bedrock\", region_name=\"us-east-1\")\n",
    "boto3_bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "# Create embedding\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\", client=boto3_bedrock_runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0b5ab6-cde6-454d-a040-b2c5f8fff328",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "Split training and testing data for prompt optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737452d0-cf32-4007-991e-12a795924a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "INPUT_FILE = 'cuad_test_50p.csv'\n",
    "df_input = pd.read_csv('./data/'+INPUT_FILE, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f94249-7651-48f4-af7a-1616c1ec37e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_train, df_input_test = train_test_split(df_input, test_size=0.2, random_state=24)\n",
    "\n",
    "df_input_train = df_input_train.reset_index()\n",
    "df_input_test = df_input_test.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa50982-9c05-4a58-ad6b-fc8732daf149",
   "metadata": {},
   "source": [
    "### Evaluation before migration\n",
    "\n",
    "Baselining the evaluation metrics for the model before migration\n",
    "\n",
    "* **BENCH_KEY**: a unique keyname for your benchmarking in this round \n",
    "* **S3_BUCKET**: the S3 buckt you created for the benchmarking    \n",
    "* **TASK_FOLDER**: the task folder you created under the S3 bucket   \n",
    "* **INPUT_FILE**: the file name of the dataset you prepared for benchmarking\n",
    "* **DOC_FILE**: the document to query \n",
    "* **VDB_NAME**: the vector db name for RAG pipeline   \n",
    "* **METRICS_LIST**: the metrics we provide for the question-answer task    \n",
    "* **BEDROCK_REGION**: the AWS region that the model benchmarking runs on Bedrock\n",
    "* **COST_FILE**: the price file used for calculating model inference cost    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea62a19-e991-422a-8ec1-f9d65b25a043",
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCH_KEY = 'PeccyMig_202503_test'\n",
    "S3_BUCKET = 'genai-sdo-llm-ben-20240310'\n",
    "TASK_FOLDER = 'ben-qa'\n",
    "INPUT_FILE = 'cuad_10.csv'\n",
    "DOC_FILE = 'PACIRA_PHARMACEUTICALS_AGREEMENT.PDF'\n",
    "VDB_NAME = \"loha\"\n",
    "METRICS_LIST = ['Inference_Time','Input_Token','Output_Token','Throughput','RougeL-sum','Semantic_Similarity','BERT-F1','Toxicity',\n",
    "                                            'Answer_Correctness','Answer_similarity','Answer_Relevancy','Context_Recall','Context_Precision','Cost','Cache_Input_Token','Cache_Output_Token']\n",
    "BEDROCK_REGION = 'us-east-1'\n",
    "COST_FILE = 'bedrock_od_public.csv'\n",
    "\n",
    "Results_qa = pd.DataFrame()\n",
    "Results_qa = Results_qa.assign(metric_name=METRICS_LIST) \n",
    "Results_qa = Results_qa.set_index('metric_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c140134-0247-4e92-abd5-9e18d071c231",
   "metadata": {},
   "source": [
    "#### Create a vector DB for RAG pipeline\n",
    "\n",
    "You can skip this step if you already created a vector db in the previous round of benchmarking\n",
    "\n",
    "* Embedding model: here we use Titan text embedding model on Bedrock\n",
    "* Set the chunk size and overlap size for your retriever embedding\n",
    "* Invoke **QA_Create_VDB** to create a faiss vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dfd588-3d19-4000-ae5b-868bbb1f91c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set chunk size and create the vector db\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 100\n",
    "\n",
    "QA_Create_VDB(chunk_size,chunk_overlap,bedrock_embeddings,VDB_NAME,S3_BUCKET,DOC_FILE,TASK_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8f634e-00aa-42d6-b595-c3da496aacff",
   "metadata": {},
   "source": [
    "Create DSPy dataset from DataFrame for classification task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf9d943-5193-4e3a-80ff-6c2c5e7fcc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "def create_qa_dataset(prompt_template,df):\n",
    "    \"\"\"\n",
    "    Create DSPy dataset from DataFrame for question-answer task \n",
    "    Args: \n",
    "        prompt_template: the prompt template for the question-answer task defined by user\n",
    "        df: input dataset in DataFrame format\n",
    "    Return: DSPy dataset\n",
    "    \"\"\"        \n",
    "    \n",
    "    dspy_dataset = []\n",
    "\n",
    "    # create embedding\n",
    "    bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\", client=boto3_bedrock_runtime)\n",
    "    \n",
    "    # load vectordb \n",
    "    vectorstore_faiss_general = FAISS.load_local(VDB_NAME, bedrock_embeddings,allow_dangerous_deserialization=True)\n",
    "    wrapper_store_faiss_general = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss_general)\n",
    "    \n",
    "    # vectordb search\n",
    "    SEARCH_K = 5\n",
    "    retriever = vectorstore_faiss_general.as_retriever(search_kwargs={\n",
    "        'k': SEARCH_K,\n",
    "    })\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        docs = retriever.get_relevant_documents(df['instruction'][i])\n",
    "        contexts = []\n",
    "        for k in range(len(docs)):\n",
    "            context_text = docs[k].page_content\n",
    "            contexts.append(context_text)\n",
    "        \n",
    "        example = dspy.Example(\n",
    "            question = prompt_template.format(question=df['instruction'][i],context=contexts),\n",
    "            answer = df['response'][i]\n",
    "        ).with_inputs(\"question\")\n",
    "\n",
    "        dspy_dataset.append(example)\n",
    "        #print(len(dspy_dataset))\n",
    "        \n",
    "    return dspy_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec96e6ac-21db-4054-861e-78332aa28e3f",
   "metadata": {},
   "source": [
    "#### Task specific setting\n",
    "\n",
    "* Configure your **prompt** in the prompt catalog (prompt_catalog.json), and configure the prompt_catalog_id\n",
    "* Set the **LLM hyperparameter** in model_kwargs. For the models on Bedrock, refer to [inferenceConfig](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-call.html)\n",
    "* Set **LLM-judge model** for RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaa5839-7a3c-4800-9416-9d073e83d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_catalog_id = \"qa-1\"\n",
    "\n",
    "model_kwargs = {\n",
    "        'maxTokens': 1024, \n",
    "        'topP': 0.9, \n",
    "        'temperature': 0\n",
    "}   \n",
    "judge_model_id = \"mistral.mistral-large-2402-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e44ff22-09f5-4206-a2a0-c900a3950359",
   "metadata": {},
   "source": [
    "#### Specify the model and other settings for benchmarking\n",
    "\n",
    "Invoke **QA_Ben** function to conduct the benchmarking for one selected model\n",
    "\n",
    "* **method**: set \"Bedrock\" for the models on Bedrock\n",
    "* **region**: configured in the previous step\n",
    "* **model_id**: specify the Model ID for the model endpoint\n",
    "* **judge_model_id**: specify LLM-judge model for RAGAS\n",
    "* **model_kwargs**: configured in previous step\n",
    "* **prompt_template**: prompt template based on the prompt configured in previous step\n",
    "* **s3_bucket**: configured in previous step\n",
    "* **file_name**: configured in previous step\n",
    "* **BENCH_KEY**: configured in previous step\n",
    "* **task_folder**: configured in previous step\n",
    "* **cost_key**: set \"public\" when using AWS public pricing to calculate the cost\n",
    "* **save_id**: the model name displayed in the report \n",
    "* **SLEEP_SEC**: you can configure \"sleep and retry\" when throtting, for example, set SLEEP_SEC = 10 to wait for 10 seconds between each inference\n",
    "* **SAMPLE_LEN**: you can configure the number of samples for inference\n",
    "* **PP_TIME**: if you want to run model inference for multiple rounds, set the number of rounds here.  \n",
    "* **cacheconf**: set \"default\" to enable Bedrock Prompt Caching in the inference, \"None\" to disable\n",
    "* **latencyOpt**: set \"optimized\" to enable Bedrock Latency Optimized Inference, \"None\" to disable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9596c4c-92be-4860-92c8-e5f81991fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEFORE_MODEL_ID = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "before_save_id = 'Haiku-3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186d2bd6-51a5-4820-9907-135f976cb13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = Prompt_Template_Gen(BEFORE_MODEL_ID, prompt_catalog_id)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030303da-dc47-489e-b31b-4b2298aa3497",
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_qa[before_save_id] = QA_Ben(method=\"Bedrock\",\n",
    "                                   region=BEDROCK_REGION,\n",
    "                                   model_id=BEFORE_MODEL_ID,\n",
    "                                   judge_model_id=judge_model_id,\n",
    "                                   model_kwargs=model_kwargs,\n",
    "                                   prompt_template=prompt_template,\n",
    "                                   vdb_name=VDB_NAME,\n",
    "                                   s3_bucket=S3_BUCKET,\n",
    "                                   file_name=INPUT_FILE,\n",
    "                                   BENCH_KEY=BENCH_KEY,\n",
    "                                   task_folder=TASK_FOLDER,\n",
    "                                   cost_key=COST_FILE,\n",
    "                                   save_id=before_save_id,\n",
    "                                   SLEEP_SEC=20,\n",
    "                                   SAMPLE_LEN=5,          # len(df_input_test),,\n",
    "                                   PP_TIME=1,\n",
    "                                   cacheconf=\"None\",latencyOpt=\"None\")\n",
    "Results_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b256e3c9-8f6f-428d-b907-6914a8e254f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce0c1e5d-38e2-4f7e-aa99-ce58d01483e4",
   "metadata": {},
   "source": [
    "### Migration starts here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3545f100-513b-4d9c-9b0e-b0e3e2eacd98",
   "metadata": {},
   "source": [
    "* **MODEL_ID**: specify the target model for migration\n",
    "* **opt_model_id**: Bedrock model ID for optimizer\n",
    "* **OPT_ITERATION**: specify the iteration number for target model prompt optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a579fb1b-dfda-4467-be82-5c24eae9fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = model_id = 'us.amazon.nova-lite-v1:0' \n",
    "opt_model_id = 'amazon.nova-lite-v1:0'\n",
    "OPT_ITERATION = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b3c967-075c-47e4-8e75-90741e3f428c",
   "metadata": {},
   "source": [
    "#### Step 1: Bedrock APO \n",
    "\n",
    "Run Bedrock APO to get optimized prompt for the target model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bc7d60-6de9-4734-a23c-33a424f34e32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#apo_prompt_template = Prompt_Opt_Template_Gen('us-west-2', opt_model_id, prompt_template)\n",
    "\n",
    "# Alternatively you can manually create the prompt following the Nova model prompt best practices\n",
    "apo_prompt_template = \"\"\"\n",
    "## General Instructions\n",
    "You are an intelligent question answering bot. Your role is to provide concise and accurate answers based on the given context and question. Carefully read and understand the provided context, then formulate your response to directly answer the question without any preamble or additional explanations.\n",
    "\n",
    "- If you do not have enough information to determine the answer, simply respond with \"I don't know\" within the <results> tags.\n",
    "- Do not make up answers. Only provide factual information derived from the given context.\n",
    "- Be brief and concise in your response.\n",
    "\n",
    "## Context\n",
    "{{context}}\n",
    "\n",
    "## Question\n",
    "{{question}}\n",
    "\n",
    "## Response Format\n",
    "<results>\n",
    "[Your answer here]\n",
    "</results>\n",
    "\"\"\"\n",
    "\n",
    "# Check the prompt output and manually update the prompt language and formatting\n",
    "apo_prompt_template = apo_prompt_template.replace(\"{{context}}\", \"{context}\")\n",
    "apo_prompt_template = apo_prompt_template.replace(\"{{question}}\", \"{question}\")\n",
    "\n",
    "print(apo_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba88dce-3e8c-416d-bc92-81eeae58adfd",
   "metadata": {},
   "source": [
    "Evaluate the target model performance using **QA_Ben** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af16a158-e7a3-4b66-9a93-1394bcd815b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_id = 'nova-lite(apo)'\n",
    "Results_qa[save_id] = QA_Ben(method=\"Bedrock\",\n",
    "                                   region=BEDROCK_REGION,\n",
    "                                   model_id=MODEL_ID,\n",
    "                                   judge_model_id=judge_model_id,\n",
    "                                   model_kwargs=model_kwargs,\n",
    "                                   prompt_template=prompt_template,\n",
    "                                   vdb_name=VDB_NAME,\n",
    "                                   s3_bucket=S3_BUCKET,\n",
    "                                   file_name=INPUT_FILE,\n",
    "                                   BENCH_KEY=BENCH_KEY,\n",
    "                                   task_folder=TASK_FOLDER,\n",
    "                                   cost_key=COST_FILE,\n",
    "                                   save_id=save_id,\n",
    "                                   SLEEP_SEC=20,\n",
    "                                   SAMPLE_LEN=5,          # len(df_input_test),,\n",
    "                                   PP_TIME=1,\n",
    "                                   cacheconf=\"None\",latencyOpt=\"None\")\n",
    "Results_qa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f1cbf7-3bb0-48c6-8d8b-324aa68aa4cf",
   "metadata": {},
   "source": [
    "#### Step 2: data-aware optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26ca4f4-4dfa-434b-891c-3537fe8a03dc",
   "metadata": {},
   "source": [
    "Prepare training dataset and model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a391011-feca-495c-9852-e08e166aaebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = create_qa_dataset(apo_prompt_template,df_input_train)\n",
    "model = model_initialize(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c79a098-be25-496f-9d33-19022b6cfd62",
   "metadata": {},
   "source": [
    "Data-aware optimization process: you can specify the following parameters for the optimizer \n",
    "\n",
    "* **model**: the initialized target model in the previous step\n",
    "* **train_set**: training dataset prepared in the previous step \n",
    "* **eval_metric_accuracy**: evaluation function \n",
    "* **num_candidates**: number of prompt candidate to generate and evaluate by the optimizer\n",
    "* **num_trials**: number of optimization trials to run by the optimizer\n",
    "* **minibatch_size**: optimize and evaluate prompt candidates over minibatch (subset of the full training set) \n",
    "* **minibatch_full_eval_steps**: every number of steps to run full evaluation on the top averaging set of prompt candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d0a7a2-e138-4741-92ec-2c1037e1947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    for k in range(OPT_ITERATION):\n",
    "        optimized_model = data_aware_optimization(model,train_set,eval_metric_ss,\n",
    "                                                num_candidates = 5,\n",
    "                                                num_trials = 7,\n",
    "                                                minibatch_size = 20,\n",
    "                                                minibatch_full_eval_steps = 7)\n",
    "    \n",
    "\n",
    "        print(\"========== Optimized prompt instruction ===============\")\n",
    "        print(optimized_model.prog.predict.signature.instructions)     # optimized_model.prog.predict.signature.instructions\n",
    "        print(\"=======================================================\")\n",
    "        print(\"retry \",k,\"...\") \n",
    "        if k< OPT_ITERATION-1:\n",
    "            time.sleep(60)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1202a6f-8d0d-40a0-9d76-130c72fe5cfd",
   "metadata": {},
   "source": [
    "Add the data-aware optimized prompt template to the prompt catalog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1957d824-28d0-44a7-878b-784b2b593439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the data-aware optimized prompt template to the prompt catalog \n",
    "update_prompt_catalog(prompt_catalog_id,optimized_model)\n",
    "\n",
    "dao_prompt_template = Prompt_Template_Gen(MODEL_ID, prompt_catalog_id+'-dao')\n",
    "print(dao_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8659bea-99cb-4c21-949f-f80deba0c5cf",
   "metadata": {},
   "source": [
    "Evaluate the target model performance using **QA_Ben** function. Based on the performance, you can go back to step 2 to re-run the optimization by using different training set and/or different optimizer parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd48fd3-55da-47bc-8567-f52ccb05954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_id = 'nova-micro(data-aware-opt)'\n",
    "Results_qa[save_id] = QA_Ben(method=\"Bedrock\",\n",
    "                                   region=BEDROCK_REGION,\n",
    "                                   model_id=MODEL_ID,\n",
    "                                   judge_model_id=judge_model_id,\n",
    "                                   model_kwargs=model_kwargs,\n",
    "                                   prompt_template=dao_prompt_template,\n",
    "                                   vdb_name=VDB_NAME,\n",
    "                                   s3_bucket=S3_BUCKET,\n",
    "                                   file_name=INPUT_FILE,\n",
    "                                   BENCH_KEY=BENCH_KEY,\n",
    "                                   task_folder=TASK_FOLDER,\n",
    "                                   cost_key=COST_FILE,\n",
    "                                   save_id=save_id,\n",
    "                                   SLEEP_SEC=20,\n",
    "                                   SAMPLE_LEN=5,          # len(df_input_test),,\n",
    "                                   PP_TIME=1,\n",
    "                                   cacheconf=\"None\",latencyOpt=\"None\")\n",
    "Results_qa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68810f5-ede0-45ac-a4c9-289149fecaae",
   "metadata": {},
   "source": [
    "#### Select your prompt for the target model based on the performance evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0682032a-67a2-45f2-9bb0-193c920de35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimized prompt from step 1\")\n",
    "print(apo_prompt_template)\n",
    "print(\"Optimized prompt from step 2\")\n",
    "print(dao_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9202d27f-8177-427e-9faf-4e37b88d3a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002735df-ff88-45b6-ad70-d203a343677c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
