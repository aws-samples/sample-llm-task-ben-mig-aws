{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2124a15b",
   "metadata": {},
   "source": [
    "## Benchmarking task - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e7171-432c-416d-9482-1d88dc7c7e52",
   "metadata": {},
   "source": [
    "#### Install python packages requested by benchmarking\n",
    "\n",
    "If you have not install the requested python libraries, uncomment the following command to run the installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b1dcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r ../peccyben/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645eec10",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db6f11c-05fb-4cb8-9a16-0edaf14a5f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from peccyben.sentimenttask import Sent_Ben\n",
    "from peccyben.utils import Ben_Save\n",
    "from peccyben.promptcatalog import Prompt_Template_Gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1483aa52-93d3-48ba-8da2-a9ef66557ab8",
   "metadata": {},
   "source": [
    "#### Configuration\n",
    "\n",
    "Setup your environment parameters \n",
    "\n",
    "* **BENCH_KEY**: a unique keyname for your benchmarking in this round \n",
    "* **S3_BUCKET**: the S3 buckt you created for the benchmarking    \n",
    "* **TASK_FOLDER**: the task folder you created under the S3 bucket   \n",
    "* **INPUT_FILE**: the file name of the dataset you prepared for benchmarking    \n",
    "* **METRICS_LIST**: the metrics we provide for the sentiment analysis task       \n",
    "* **BEDROCK_REGION**: the AWS region that the model benchmarking runs on Bedrock\n",
    "* **COST_FILE**: the price file used for calculating model inference cost      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b26a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCH_KEY = 'PeccyBen_202503_test'\n",
    "S3_BUCKET = 'genai-sdo-llm-ben-20240310'\n",
    "TASK_FOLDER = 'ben-sentiment'\n",
    "INPUT_FILE = 'agnews_100.csv'\n",
    "METRICS_LIST = ['Inference_Time','Input_Token','Output_Token','Throughput','Accuracy','Toxicity','Cost','Cache_Input_Token','Cache_Output_Token']\n",
    "BEDROCK_REGION = 'us-east-1'\n",
    "COST_FILE = 'bedrock_od_public.csv'\n",
    "\n",
    "Results_sent = pd.DataFrame()\n",
    "Results_sent = Results_sent.assign(metric_name=METRICS_LIST) \n",
    "Results_sent = Results_sent.set_index('metric_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc00de9d-cee5-4625-b76c-c2395160553d",
   "metadata": {},
   "source": [
    "#### Task specific setting\n",
    "\n",
    "* Configure your **prompt** in the prompt catalog (prompt_catalog.json), and configure the prompt_catalog_id\n",
    "* Set the **LLM hyperparameter** in model_kwargs. For the models on Bedrock, refer to [inferenceConfig](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-call.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c361eb6-37f2-4a30-8962-ef8b895d1286",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_catalog_id = \"sent-1\"\n",
    "\n",
    "model_kwargs = {\n",
    "        'maxTokens': 512, \n",
    "        'topP': 0.9, \n",
    "        'temperature': 0\n",
    "}   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9867236-750b-47c5-8da8-a8706881e4a8",
   "metadata": {},
   "source": [
    "#### Specify the model and other settings for benchmarking\n",
    "\n",
    "Invoke **Sent_Ben** function to conduct the benchmarking for one selected model, repeat for multiple models you want to benchmark\n",
    "\n",
    "* **method**: set \"Bedrock\" for the models on Bedrock\n",
    "* **region**: configured in the previous step\n",
    "* **model_id**: specify the Model ID for the model endpoint\n",
    "* **model_kwargs**: configured in previous step\n",
    "* **prompt_template**: prompt template based on the prompt configured in previous step\n",
    "* **s3_bucket**: configured in previous step\n",
    "* **file_name**: configured in previous step\n",
    "* **BENCH_KEY**: configured in previous step\n",
    "* **task_folder**: configured in previous step\n",
    "* **cost_key**: set \"public\" when using AWS public pricing to calculate the cost\n",
    "* **save_id**: the model name displayed in the report \n",
    "* **SLEEP_SEC**: you can configure \"sleep and retry\" when throtting, for example, set SLEEP_SEC = 10 to wait for 10 seconds between each inference\n",
    "* **SAMPLE_LEN**: you can configure the number of samples for inference\n",
    "* **PP_TIME**: if you want to run model inference for multiple rounds, set the number of rounds here.  \n",
    "* **cacheconf**: set \"default\" to enable Bedrock Prompt Caching in the inference, \"None\" to disable\n",
    "* **latencyOpt**: set \"optimized\" to enable Bedrock Latency Optimized Inference, \"None\" to disable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b8cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nova-micro \n",
    "model_id = 'amazon.nova-micro-v1:0' \n",
    "save_id = 'nova-micro'\n",
    "\n",
    "prompt_template = Prompt_Template_Gen(model_id, prompt_catalog_id)\n",
    "#print(prompt_template)\n",
    "\n",
    "Results_sent[model_id] = Sent_Ben(method=\"Bedrock\",\n",
    "                                   region=BEDROCK_REGION,\n",
    "                                   model_id=model_id,\n",
    "                                   model_kwargs=model_kwargs,\n",
    "                                   prompt_template=prompt_template,\n",
    "                                   s3_bucket=S3_BUCKET,\n",
    "                                   file_name=INPUT_FILE,\n",
    "                                   BENCH_KEY=BENCH_KEY,\n",
    "                                   task_folder=TASK_FOLDER,\n",
    "                                   cost_key=COST_FILE,\n",
    "                                   save_id=save_id,\n",
    "                                   SLEEP_SEC=2,SAMPLE_LEN=2,\n",
    "                                   PP_TIME=2,\n",
    "                                   cacheconf=\"None\",latencyOpt=\"None\")\n",
    "Results_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b7fea9-4c97-4581-9685-989f4b89d686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.5 Haiku\n",
    "model_id = 'us.anthropic.claude-3-5-haiku-20241022-v1:0' \n",
    "save_id = 'Haiku-3.5'\n",
    "\n",
    "prompt_template = Prompt_Template_Gen(model_id, prompt_catalog_id)\n",
    "#print(prompt_template)\n",
    "\n",
    "Results_sent[model_id] = Sent_Ben(method=\"Bedrock\",\n",
    "                                   region=BEDROCK_REGION,\n",
    "                                   model_id=model_id,\n",
    "                                   model_kwargs=model_kwargs,\n",
    "                                   prompt_template=prompt_template,\n",
    "                                   s3_bucket=S3_BUCKET,\n",
    "                                   file_name=INPUT_FILE,\n",
    "                                   BENCH_KEY=BENCH_KEY,\n",
    "                                   task_folder=TASK_FOLDER,\n",
    "                                   cost_key=COST_FILE,\n",
    "                                   save_id=save_id,\n",
    "                                   SLEEP_SEC=2,SAMPLE_LEN=2,\n",
    "                                   PP_TIME=2,\n",
    "                                   cacheconf=\"None\",latencyOpt=\"None\")\n",
    "Results_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4042cb09-d07e-49ec-a421-98dec169ad83",
   "metadata": {},
   "source": [
    "#### Generate benchmarking report\n",
    "\n",
    "Invoke **Ben_Save** function to generate your benchmarking report and cost-performance analysis, all the results are stored in S3 bucket\n",
    "\n",
    "* **Results_sent**: the benchmarking results generated in previous step\n",
    "* **S3_BUCKET**: configured in previous step\n",
    "* **BENCH_KEY**: configured in previous step\n",
    "* **TASK_FOLDER**: configured in previous step\n",
    "* **perf_metric**: select the performance metric from the metrics list in the previous step, for cost-performance analysis, for example, to analyze the accuracy score with cost, set \"Accuracy\".  \n",
    "* **top_x**: set the top x number of models you want to run the cost-performance analysis \n",
    "* **TITLE**: specify a title for the reports and charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a560ef4-c86f-420a-9e99-f1f66951cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_metric = 'Accuracy'\n",
    "top_x = 5\n",
    "\n",
    "Ben_Save(Results_sent,S3_BUCKET,BENCH_KEY,TASK_FOLDER,perf_metric,top_x,TITLE=\"Sentiment-Task\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e2d5b6-0a23-4c29-8f82-d4d4496dcbad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38378fed-e59c-4820-b973-1e5cfe691be2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
