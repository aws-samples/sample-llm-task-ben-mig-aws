{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "212edb18-fefb-4cd8-b16e-1977fd3eba9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LLM migration driven by data-aware prompt optimization - Summarization task\n",
    "\n",
    "Example notebook \n",
    "- model migration from Claude 3 Haiku to Nova-Lite for a summarization task\n",
    "- optimization by Bedrock APO and data-aware optimization after migration\n",
    "- evaluation metric: LJ-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562d489e-dd83-4ca5-9cc7-562b9c843192",
   "metadata": {},
   "source": [
    "### Install python packages requested by benchmarking\n",
    "\n",
    "If you have not install the requested python libraries, uncomment the following command to run the installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04a0083-fc55-4630-a1ca-20437b4d8057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r ../peccyben/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25536b8-4f4f-490f-9824-4b91bc0340bf",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5c911c-4fea-4779-8c70-c8e78308814e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os, time\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json, copy\n",
    "import warnings \n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from peccymig.migration import Prompt_Opt_Template_Gen, lj_metric_summ\n",
    "from peccymig.migration import model_initialize, model_inference, data_aware_optimization, update_prompt_catalog\n",
    "from peccyben.summarizationtask import Summ_Ben \n",
    "from peccyben.promptcatalog import Prompt_Template_Gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0b5ab6-cde6-454d-a040-b2c5f8fff328",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "Split training and testing data for prompt optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737452d0-cf32-4007-991e-12a795924a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "INPUT_FILE = 'xsum_100.csv'\n",
    "df_input = pd.read_csv('./data/'+INPUT_FILE, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f94249-7651-48f4-af7a-1616c1ec37e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_train, df_input_test = train_test_split(df_input, test_size=0.9, random_state=42)\n",
    "\n",
    "df_input_train = df_input_train.reset_index()\n",
    "df_input_test = df_input_test.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df19d1a-7b8d-4b11-b27c-167a3bc782b0",
   "metadata": {},
   "source": [
    "Create DSPy dataset from DataFrame for classification task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad18c722-5d9b-461d-9c71-e19953e874ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "def create_summ_dataset(prompt_template,df):\n",
    "    \"\"\"\n",
    "    Create DSPy dataset from DataFrame for summarization task \n",
    "    Args: \n",
    "        prompt_template: the prompt template for the summarization task defined by user\n",
    "        df: input dataset in DataFrame format\n",
    "    Return: DSPy dataset\n",
    "    \"\"\"        \n",
    "    \n",
    "    dspy_dataset = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        example = dspy.Example(\n",
    "            question = prompt_template.format(document=df['Section_text'][i]),\n",
    "            answer=df['Section_text'][i]\n",
    "        ).with_inputs(\"question\")\n",
    "\n",
    "        dspy_dataset.append(example)\n",
    "        #print(len(dspy_dataset))\n",
    "        \n",
    "    return dspy_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa50982-9c05-4a58-ad6b-fc8732daf149",
   "metadata": {},
   "source": [
    "### Evaluation before migration\n",
    "\n",
    "Baselining the evaluation metrics for the model before migration\n",
    "\n",
    "* **BENCH_KEY**: a unique keyname for your benchmarking in this round \n",
    "* **S3_BUCKET**: the S3 buckt you created for the benchmarking    \n",
    "* **TASK_FOLDER**: the task folder you created under the S3 bucket   \n",
    "* **INPUT_FILE**: the file name of the dataset you prepared for benchmarking    \n",
    "* **METRICS_LIST**: the metrics we provide for the text classification task   \n",
    "* **BEDROCK_REGION**: the AWS region that the model benchmarking runs on Bedrock\n",
    "* **COST_FILE**: the price file used for calculating model inference cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea62a19-e991-422a-8ec1-f9d65b25a043",
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCH_KEY = 'PeccyMig_202503_test'\n",
    "S3_BUCKET = 'genai-sdo-llm-ben-20240310'\n",
    "TASK_FOLDER = 'ben-summ'\n",
    "INPUT_FILE = 'xsum_20.csv'\n",
    "METRICS_LIST = ['Inference_Time','Input_Token','Output_Token','Throughput','RougeL-sum','Semantic_Similarity','BERT-F1','LJ_Score','Toxicity','Cost','Cache_Input_Token','Cache_Output_Token']\n",
    "BEDROCK_REGION = 'us-east-1'\n",
    "COST_FILE = 'bedrock_od_public.csv'\n",
    "\n",
    "Results_summ = pd.DataFrame()\n",
    "Results_summ = Results_summ.assign(metric_name=METRICS_LIST) \n",
    "Results_summ = Results_summ.set_index('metric_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8abe1f-93ff-4795-a1f0-eef215ec4aaa",
   "metadata": {},
   "source": [
    "#### Task specific setting\n",
    "\n",
    "* Configure your **prompt** in the prompt catalog (prompt_catalog.json), and configure the prompt_catalog_id\n",
    "* Set the **LLM hyperparameter** in model_kwargs. For the models on Bedrock, refer to [inferenceConfig](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-call.html)\n",
    "* Set two **LLM-judge models** (judge_model_1, judge_model_2) for the LLM-judge for the summarization task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaa5839-7a3c-4800-9416-9d073e83d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_catalog_id = \"summ-1\"\n",
    "\n",
    "model_kwargs = {\n",
    "    'maxTokens': 512, \n",
    "    'topP': 0.9, \n",
    "    'temperature': 0\n",
    "}   \n",
    "\n",
    "judge_model_1 = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "judge_model_2 = \"us.deepseek.r1-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907014bd-878b-4ff8-82f2-bcbe15b54fde",
   "metadata": {},
   "source": [
    "#### Specify the model and other settings for benchmarking\n",
    "\n",
    "Invoke **Summ_Ben** function to conduct the benchmarking for one selected model\n",
    "\n",
    "* **method**: set \"Bedrock\" for the models on Bedrock\n",
    "* **region**: configured in the previous step\n",
    "* **model_id**: specify the Model ID for the model endpoint\n",
    "* **model_kwargs**: configured in previous step\n",
    "* **prompt_template**: prompt template based on the prompt configured in previous step\n",
    "* **s3_bucket**: configured in previous step\n",
    "* **file_name**: configured in previous step\n",
    "* **BENCH_KEY**: configured in previous step\n",
    "* **task_folder**: configured in previous step\n",
    "* **cost_key**: set \"public\" when using AWS public pricing to calculate the cost\n",
    "* **save_id**: the model name displayed in the report \n",
    "* **SLEEP_SEC**: you can configure \"sleep and retry\" when throtting, for example, set SLEEP_SEC = 10 to wait for 10 seconds between each inference\n",
    "* **SAMPLE_LEN**: you can configure the number of samples for inference\n",
    "* **PP_TIME**: if you want to run model inference for multiple rounds, set the number of rounds here.  \n",
    "* **cacheconf**: set \"default\" to enable Bedrock Prompt Caching in the inference, \"None\" to disable\n",
    "* **latencyOpt**: set \"optimized\" to enable Bedrock Latency Optimized Inference, \"None\" to disable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9596c4c-92be-4860-92c8-e5f81991fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEFORE_MODEL_ID = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "before_save_id = 'haiku-3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186d2bd6-51a5-4820-9907-135f976cb13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = Prompt_Template_Gen(BEFORE_MODEL_ID, prompt_catalog_id)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030303da-dc47-489e-b31b-4b2298aa3497",
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_summ[before_save_id] = Summ_Ben(method=\"Bedrock\",\n",
    "                                 region=BEDROCK_REGION,\n",
    "                                 model_id=BEFORE_MODEL_ID,\n",
    "                                 jm1=judge_model_1,\n",
    "                                 jm2=judge_model_2,\n",
    "                                 model_kwargs=model_kwargs,\n",
    "                                 prompt_template=prompt_template,\n",
    "                                 s3_bucket=S3_BUCKET,\n",
    "                                 file_name=INPUT_FILE,\n",
    "                                 BENCH_KEY=BENCH_KEY,\n",
    "                                 task_folder=TASK_FOLDER,\n",
    "                                 cost_key=COST_FILE,\n",
    "                                 save_id=before_save_id,\n",
    "                                 SLEEP_SEC=20,\n",
    "                                 SAMPLE_LEN=3,   #len(df_input_test),\n",
    "                                 PP_TIME=1,\n",
    "                                 cacheconf=\"None\",latencyOpt=\"None\")\n",
    "\n",
    "Results_summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b256e3c9-8f6f-428d-b907-6914a8e254f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce0c1e5d-38e2-4f7e-aa99-ce58d01483e4",
   "metadata": {},
   "source": [
    "### Migration starts here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1015892a-e364-492b-b5bd-a758ac0debe8",
   "metadata": {},
   "source": [
    "* **MODEL_ID**: specify the target model for migration\n",
    "* **opt_model_id**: Bedrock model ID for optimizer\n",
    "* **OPT_ITERATION**: specify the iteration number for target model prompt optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a579fb1b-dfda-4467-be82-5c24eae9fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = 'us.amazon.nova-lite-v1:0'\n",
    "opt_model_id = 'amazon.nova-lite-v1:0'\n",
    "OPT_ITERATION = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b3c967-075c-47e4-8e75-90741e3f428c",
   "metadata": {},
   "source": [
    "#### Step 1: Bedrock APO \n",
    "\n",
    "Run Bedrock APO to get optimized prompt for the target model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bc7d60-6de9-4734-a23c-33a424f34e32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#apo_prompt_template = Prompt_Opt_Template_Gen('us-west-2', opt_model_id, prompt_template)\n",
    "\n",
    "# Alternatively you can manually create the prompt following the Nova model prompt best practices\n",
    "apo_prompt_template = \"\"\"\n",
    "## Task\n",
    "Your task is to summarize the given document enclosed in <doc></doc> tags in a brief and concise manner, without adding any information not mentioned in the document. Do not provide a preamble - start directly with the summarization.\n",
    "\n",
    "## Guidelines\n",
    "- Read the document carefully to understand its main points and key information.\n",
    "- Identify the core ideas, facts, and arguments presented in the document.\n",
    "- Synthesize the essential information into a clear and succinct summary.\n",
    "- Use your own words to paraphrase the key points from the document.\n",
    "- Omit unnecessary details or examples to keep the summary focused on the central concepts.\n",
    "- If you cannot summarize the document, simply respond \"I don't know\" without making up an answer.\n",
    "\n",
    "## Document to Summarize\n",
    "<doc>\n",
    "{{document}}\n",
    "</doc>\n",
    "\n",
    "Please provide your concise summary immediately without any preamble:\"\"\"\n",
    "\n",
    "# Check the prompt output and manually update the prompt language and formatting \n",
    "apo_prompt_template = apo_prompt_template.replace(\"{{document}}\", \"{document}\")\n",
    "\n",
    "print(apo_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d98a0c-bbe1-4434-a901-5d8ffee804cd",
   "metadata": {},
   "source": [
    "Evaluate the target model performance using **Summ_Ben** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af16a158-e7a3-4b66-9a93-1394bcd815b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_id = 'nova-lite(apo)'\n",
    "Results_summ[save_id] = Summ_Ben(method=\"Bedrock\",\n",
    "                                 region=BEDROCK_REGION,\n",
    "                                 model_id=MODEL_ID,\n",
    "                                 jm1=judge_model_1,\n",
    "                                 jm2=judge_model_2,\n",
    "                                 model_kwargs=model_kwargs,\n",
    "                                 prompt_template=apo_prompt_template,\n",
    "                                 s3_bucket=S3_BUCKET,\n",
    "                                 file_name=INPUT_FILE,\n",
    "                                 BENCH_KEY=BENCH_KEY,\n",
    "                                 task_folder=TASK_FOLDER,\n",
    "                                 cost_key=COST_FILE,\n",
    "                                 save_id=save_id,\n",
    "                                 SLEEP_SEC=20,\n",
    "                                 SAMPLE_LEN=3,   #len(df_input_test),\n",
    "                                 PP_TIME=1,\n",
    "                                 cacheconf=\"None\",latencyOpt=\"None\")\n",
    "\n",
    "Results_summ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f1cbf7-3bb0-48c6-8d8b-324aa68aa4cf",
   "metadata": {},
   "source": [
    "#### Step 2: data-aware optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c416fb-72b9-4e2f-984f-694748aee9ea",
   "metadata": {},
   "source": [
    "Prepare training dataset and model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a391011-feca-495c-9852-e08e166aaebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = create_summ_dataset(apo_prompt_template,df_input_train)\n",
    "model = model_initialize(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7780a3-e8df-42f1-9be0-b7a3a5748147",
   "metadata": {},
   "source": [
    "Data-aware optimization process: you can specify the following parameters for the optimizer \n",
    "\n",
    "* **model**: the initialized target model in the previous step\n",
    "* **train_set**: training dataset prepared in the previous step \n",
    "* **eval_metric_accuracy**: evaluation function \n",
    "* **num_candidates**: number of prompt candidate to generate and evaluate by the optimizer\n",
    "* **num_trials**: number of optimization trials to run by the optimizer\n",
    "* **minibatch_size**: optimize and evaluate prompt candidates over minibatch (subset of the full training set) \n",
    "* **minibatch_full_eval_steps**: every number of steps to run full evaluation on the top averaging set of prompt candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d0a7a2-e138-4741-92ec-2c1037e1947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    for k in range(OPT_ITERATION):\n",
    "        optimized_model = data_aware_optimization(model,train_set,lj_metric_summ,\n",
    "                                                num_candidates = 5,\n",
    "                                                num_trials = 7,\n",
    "                                                minibatch_size = 5,\n",
    "                                                minibatch_full_eval_steps = 7)\n",
    "    \n",
    "\n",
    "        print(\"========== Optimized prompt instruction ===============\")\n",
    "        print(optimized_model.prog.predict.signature.instructions)     # optimized_model.prog.predict.signature.instructions\n",
    "        print(\"=======================================================\")\n",
    "        print(\"retry \",k,\"...\") \n",
    "        if k< OPT_ITERATION-1:\n",
    "            time.sleep(60)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c186e8f4-d97b-46b6-898f-0a9863777ff2",
   "metadata": {},
   "source": [
    "Add the data-aware optimized prompt template to the prompt catalog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1957d824-28d0-44a7-878b-784b2b593439",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_prompt_catalog(prompt_catalog_id,optimized_model)\n",
    "\n",
    "dao_prompt_template = Prompt_Template_Gen(MODEL_ID, prompt_catalog_id+'-dao')\n",
    "print(dao_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51501e0-cb6b-4693-809f-56cf7542e7a7",
   "metadata": {},
   "source": [
    "Evaluate the target model performance using **Summ_Ben** function. Based on the performance, you can go back to step 2 to re-run the optimization by using different training set and/or different optimizer parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd48fd3-55da-47bc-8567-f52ccb05954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_id = 'nova-lite(data-aware-opt)'\n",
    "Results_summ[save_id] = Summ_Ben(method=\"Bedrock\",\n",
    "                                 region=BEDROCK_REGION,\n",
    "                                 model_id=MODEL_ID,\n",
    "                                 jm1=judge_model_1,\n",
    "                                 jm2=judge_model_2,\n",
    "                                 model_kwargs=model_kwargs,\n",
    "                                 prompt_template=dao_prompt_template,\n",
    "                                 s3_bucket=S3_BUCKET,\n",
    "                                 file_name=INPUT_FILE,\n",
    "                                 BENCH_KEY=BENCH_KEY,\n",
    "                                 task_folder=TASK_FOLDER,\n",
    "                                 cost_key=COST_FILE,\n",
    "                                 save_id=save_id,\n",
    "                                 SLEEP_SEC=20,\n",
    "                                 SAMPLE_LEN=3,   #len(df_input_test),\n",
    "                                 PP_TIME=1,\n",
    "                                 cacheconf=\"None\",latencyOpt=\"None\")\n",
    "\n",
    "Results_summ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728502f3-acfa-4b0d-8358-76a22e84b1a1",
   "metadata": {},
   "source": [
    "#### Select your prompt for the target model based on the performance evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9202d27f-8177-427e-9faf-4e37b88d3a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==== Optimized prompt from step 1 =====\")\n",
    "print(apo_prompt_template)\n",
    "print(\"\\n===== Optimized prompt from step 2 =====\")\n",
    "print(dao_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ba04b5-8c07-445b-b620-0c96393c5bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9032a2-7cbf-41dd-ab75-b52b597c6d85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
